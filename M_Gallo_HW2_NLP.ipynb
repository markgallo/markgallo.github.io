{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fake-useragent\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
      "Building wheels for collected packages: fake-useragent\n",
      "  Building wheel for fake-useragent (setup.py): started\n",
      "  Building wheel for fake-useragent (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Mark\\AppData\\Local\\pip\\Cache\\wheels\\5e\\63\\09\\d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
      "Successfully built fake-useragent\n",
      "Installing collected packages: fake-useragent\n",
      "Successfully installed fake-useragent-0.1.11\n"
     ]
    }
   ],
   "source": [
    "#!pip install PyYAML\n",
    "#!pip install ua-parser\n",
    "#!pip install user-agents\n",
    "!pip install fake-useragent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (10 points) Change the variable my_path to reflect a directory on your laptop where you will store the crawled websites and enter in ‘qmss columbia’ as the variable called ‘the_query’, then run the program (with my_top.py open run)\n",
    "1. Perform the same search using googles search engine\n",
    "2. Perform a quick comparison between the google crawl results and the outputted text files. What are some observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *1.2. The crawl results provide the urls that one could leverage to get to the websites associated with a \"qmss columbia\" google search. While the outputted txt files provide the scraped text found on each website. So if the crawler output is: http://qmss.squarespace.com/ ; then the associated txt file will contain the text from that website.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crawler(object):\n",
    "\n",
    "    def my_scraper(self, tmp_url_in):\n",
    "        from bs4 import BeautifulSoup\n",
    "        import requests\n",
    "        import re\n",
    "        tmp_text = ''\n",
    "        try:\n",
    "            content = requests.get(tmp_url_in)\n",
    "            soup = BeautifulSoup(content.text, 'html.parser')\n",
    "    \n",
    "            tmp_text = soup.findAll('p') \n",
    "    \n",
    "            tmp_text = [word.text for word in tmp_text]\n",
    "            tmp_text = ' '.join(tmp_text)\n",
    "            tmp_text = re.sub('\\W+', ' ', re.sub('xa0', ' ', tmp_text))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        return tmp_text\n",
    "    \n",
    "    def fetch_urls(self, query_tmp, cnt):\n",
    "        #now lets use the following function that returns\n",
    "        #URLs from an arbitrary regex crawl form google\n",
    "    \n",
    "        #pip install pyyaml ua-parser user-agents fake-useragent\n",
    "        import requests\n",
    "        from fake_useragent import UserAgent\n",
    "        from bs4 import BeautifulSoup\n",
    "        import re \n",
    "        ua = UserAgent()\n",
    "    \n",
    "        query = '+'.join(query_tmp.split())\n",
    "        google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(cnt)\n",
    "        print (google_url)\n",
    "        response = requests.get(google_url, {\"User-Agent\": ua.random})\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        result_div = soup.find_all('div', attrs = {'class': 'ZINbbc'})\n",
    "\n",
    "        links = []\n",
    "        titles = []\n",
    "        descriptions = []\n",
    "        for r in result_div:\n",
    "            # Checks if each element is present, else, raise exception\n",
    "            try:\n",
    "                link = r.find('a', href = True)\n",
    "                title = r.find('div', attrs={'class':'vvjwJb'}).get_text()\n",
    "                description = r.find('div', attrs={'class':'s3v9rd'}).get_text()\n",
    "    \n",
    "                # Check to make sure everything is present before appending\n",
    "                if link != '' and title != '' and description != '': \n",
    "                    links.append(link['href'])\n",
    "                    titles.append(title)\n",
    "                    descriptions.append(description)\n",
    "            # Next loop if one element is not present\n",
    "            except:\n",
    "                continue  \n",
    "    \n",
    "        to_remove = []\n",
    "        clean_links = []\n",
    "        for i, l in enumerate(links):\n",
    "            clean = re.search('\\/url\\?q\\=(.*)\\&sa',l)\n",
    "    \n",
    "            # Anything that doesn't fit the above pattern will be removed\n",
    "            if clean is None:\n",
    "                to_remove.append(i)\n",
    "                continue\n",
    "            clean_links.append(clean.group(1))\n",
    "\n",
    "        return clean_links\n",
    " \n",
    "    def write_crawl_results(self, the_path, my_query, the_cnt_in):\n",
    "        #let use fetch_urls to get URLs then pass to the my_scraper function \n",
    "        import os\n",
    "        import re\n",
    "        \n",
    "        the_urls_list = self.fetch_urls(my_query, the_cnt_in)\n",
    "        try:\n",
    "            os.makedirs(the_path + re.sub('[ ]+', '_', re.sub('\"', '', my_query)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        cnt = 0\n",
    "        for word in the_urls_list:\n",
    "            tmp_txt = self.my_scraper(word)\n",
    "            if len(tmp_txt) != 0:\n",
    "                try:\n",
    "                    tmp_file = open(the_path + re.sub('[ ]+', '_', re.sub('\"', '', my_query)) + '/' + str(cnt) + '.txt',\"w\") \n",
    "                    tmp_file.write(tmp_txt)\n",
    "                    tmp_file.close()\n",
    "                    print (word)\n",
    "                    cnt += 1\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/search?q=qmss+columbia&num=50\n",
      "http://www.qmss.columbia.edu/\n",
      "https://gsas.columbia.edu/degree-programs/ma-programs/quantitative-methods-social-sciences\n",
      "http://iserp.columbia.edu/hiring-payroll/qmss-ra-program\n",
      "https://medium.com/%40RobinCRLee/review-on-columbia-qmss-ma-program-d22c0db2f0d7\n",
      "https://www.facebook.com/Quantitative-Methods-in-Social-Sciences-QMSS-Columbia-University-147741968642064/\n",
      "https://twitter.com/qmss_columbia%3Flang%3Den\n",
      "https://www.goodreads.com/group/show/80365-qmss-at-columbia-university\n",
      "https://economics.rice.edu/undergraduate-program/resources/student-opportunities/qmss-columbia-university\n",
      "https://forum.thegradcafe.com/topic/121708-masters-programs-similar-to-columbia-qmss/\n",
      "http://qmss.squarespace.com/\n",
      "https://asintunado.wordpress.com/2016/10/02/m-a-quantitative-methods-in-the-social-sciences-at-columbia-university-things-you-should-know/\n",
      "https://rdrr.io/github/jgabry/QMSS_package/man/QMSS.html\n",
      "https://www.coursicle.com/columbia/courses/QMSS/G5052/\n",
      "https://uwecon.wordpress.com/2019/01/16/columbias-qmss-ma-program/\n",
      "https://publish.illinois.edu/statadvising/2013/10/16/masters-opportunity-columbia-university-quantitative-methods-in-the-social-sciences/\n",
      "https://listserv.wustl.edu/scripts/wa.exe%3FA2%3Dind1806A%26L%3DPOLMETH%26P%3D4196\n",
      "https://maliha-tariq.com/2018/04/02/sometimes-i-write-things/\n",
      "https://www.urch.com/forums/phd-economics/118102-columbia-qmss-versus-duke-ma-econ.html\n",
      "https://barnard.edu/4plus1-pathway-quantitative-masters-social-sciences\n",
      "http://umdecon.blogspot.com/2012/10/colubmia-university-master-of-arts-in.html\n",
      "https://www.meetup.com/NYC-Data-Wranglers/messages/boards/thread/49041056\n",
      "https://www.mastersportal.com/studies/74546/quantitative-methods-in-the-social-sciences.html\n",
      "http://websites.milonic.com/qmss.columbia.edu\n",
      "https://www.poliscirumors.com/topic/chicago-macss-vs-columbia-qmss-as-phd-prep\n",
      "https://talk.collegeconfidential.com/graduate-school/1980046-help-choosing-from-graduate-programs-columbia-northeastern-and-boston-college.html\n",
      "https://dataverse.harvard.edu/dataverse/ruonan_s_thesis%3Fwidget%3Ddataverse%40harvard\n",
      "https://www.idealist.org/en/nonprofit/d919b5c9c93b4d14801689c7b99c7d11-columbia-university-quantitative-methods-in-the-social-sciences-ma-program-new-york\n",
      "https://www.poormagazine.org/node/1774\n",
      "https://www.pdffiller.com/269346691-Acquavella-Mariapdf-evaluating-the-impact-on-school-enrollment-of-venezuela-cash-transfer-program-by-maria-andrea-acquavella-form-\n",
      "http://web.horde.to/qmss.columbia.edu\n"
     ]
    }
   ],
   "source": [
    "from crawler import crawler\n",
    "\n",
    "my_path = 'D:/QMSS/Spring/NaturalLanguageProcessing/Homework/HW2/crawled_websites_'\n",
    "the_query = 'qmss columbia'\n",
    "num_docs = 50\n",
    "\n",
    "my_func = crawler()\n",
    "\n",
    "my_func.write_crawl_results(my_path, the_query, num_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. (35 points) Refactor the code to loop through 4 topics of your choice; do not run the code yet, move to step 3.  Hint: the_query will need to be an array of 4 possible queries that you loop through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = 'D:/QMSS/Spring/NaturalLanguageProcessing/Homework/HW2/crawled_websites_'\n",
    "\n",
    "ques = [\"new york mets\", \"tesla\", \"how to fry an egg\", \"sean pablo\"]\n",
    "\n",
    "def get_ques(var):\n",
    "    for v in var:\n",
    "        num_docs = 50 \n",
    "        my_func = crawler()\n",
    "        my_func.write_crawl_results(my_path, v, num_docs)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. (55 points) Refactor the file called crawler.py to not write to text files, but rather append a pandas dataframe called ‘the_data’, accessible on my_top.py, that contains four columns:\n",
    "\n",
    "1. 'body_basic' - basic removal of special characters; already coded for you\n",
    "\n",
    "2. 'body_stem' - perform stemming (Porter) on body_basic and write resultant to this column\n",
    "\n",
    "3. 'label' - what query the body relates to; if more than one word exist in the query, concatenate all words together with a ‘_’ between them all\n",
    "\n",
    "4. Upload a zip file to CourseWorks with the refactored files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crawler(object):\n",
    "\n",
    "    def my_scraper(self, tmp_url_in):\n",
    "        from bs4 import BeautifulSoup\n",
    "        import requests\n",
    "        import re\n",
    "        tmp_text = ''\n",
    "        try:\n",
    "            content = requests.get(tmp_url_in)\n",
    "            soup = BeautifulSoup(content.text, 'html.parser')\n",
    "    \n",
    "            tmp_text = soup.findAll('p') \n",
    "    \n",
    "            tmp_text = [word.text for word in tmp_text]\n",
    "            tmp_text = ' '.join(tmp_text)\n",
    "            tmp_text = re.sub('\\W+', ' ', re.sub('xa0', ' ', tmp_text))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        return tmp_text\n",
    "    \n",
    "    def fetch_urls(self, query_tmp, cnt):\n",
    "        #now lets use the following function that returns\n",
    "        #URLs from an arbitrary regex crawl form google\n",
    "    \n",
    "        #pip install pyyaml ua-parser user-agents fake-useragent\n",
    "        import requests\n",
    "        from fake_useragent import UserAgent\n",
    "        from bs4 import BeautifulSoup\n",
    "        import re \n",
    "        ua = UserAgent()\n",
    "    \n",
    "        query = '+'.join(query_tmp.split())\n",
    "        google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(cnt)\n",
    "        #print (google_url)\n",
    "        response = requests.get(google_url, {\"User-Agent\": ua.random})\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        result_div = soup.find_all('div', attrs = {'class': 'ZINbbc'})\n",
    "\n",
    "        links = []\n",
    "        titles = []\n",
    "        descriptions = []\n",
    "        for r in result_div:\n",
    "            # Checks if each element is present, else, raise exception\n",
    "            try:\n",
    "                link = r.find('a', href = True)\n",
    "                title = r.find('div', attrs={'class':'vvjwJb'}).get_text()\n",
    "                description = r.find('div', attrs={'class':'s3v9rd'}).get_text()\n",
    "    \n",
    "                # Check to make sure everything is present before appending\n",
    "                if link != '' and title != '' and description != '': \n",
    "                    links.append(link['href'])\n",
    "                    titles.append(title)\n",
    "                    descriptions.append(description)\n",
    "            # Next loop if one element is not present\n",
    "            except:\n",
    "                continue  \n",
    "    \n",
    "        to_remove = []\n",
    "        clean_links = []\n",
    "        for i, l in enumerate(links):\n",
    "            clean = re.search('\\/url\\?q\\=(.*)\\&sa',l)\n",
    "    \n",
    "            # Anything that doesn't fit the above pattern will be removed\n",
    "            if clean is None:\n",
    "                to_remove.append(i)\n",
    "                continue\n",
    "            clean_links.append(clean.group(1))\n",
    "\n",
    "        return clean_links\n",
    " \n",
    "    def write_crawl_results(self, the_path, my_query, the_cnt_in):\n",
    "        #let use fetch_urls to get URLs then pass to the my_scraper function \n",
    "        import os\n",
    "        import re\n",
    "        \n",
    "        the_urls_list = self.fetch_urls(my_query, the_cnt_in)\n",
    "        try:\n",
    "            os.makedirs(the_path + re.sub('[ ]+', '_', re.sub('\"', '', my_query)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        cnt = 0\n",
    "        \n",
    "        for word in the_urls_list:\n",
    "            tmp_txt = self.my_scraper(word)        \n",
    "            if len(tmp_txt) != 0:\n",
    "                try:\n",
    "                    tmp_file = open(the_path + re.sub('[ ]+', '_', re.sub('\"', '', my_query)) + '/' + str(cnt) + '.txt',\"w\") \n",
    "                    tmp_file.write(tmp_txt)\n",
    "                    tmp_file.close()\n",
    "                    print (word)\n",
    "                    cnt += 1\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_path = 'D:/QMSS/Spring/NaturalLanguageProcessing/Homework/HW2/Crawled/'\n",
    "\n",
    "ques = [\"new york mets\", \"tesla\", \"how to fry an egg\", \"sean pablo\"]\n",
    "\n",
    "def get_ques(var):\n",
    "    for v in var:\n",
    "        num_docs = 50\n",
    "        my_func = crawler()\n",
    "        my_func.write_crawl_results(my_path, v, num_docs)#v = the_query\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://twitter.com/Mets/status/1233520503324893186%3Fref_src%3Dtwsrc%255Egoogle%257Ctwcamp%255Eserp%257Ctwgr%255Etweet\n",
      "https://www.mlb.com/mets\n",
      "https://www.espn.com/mlb/team/_/name/nym/new-york-mets\n",
      "https://en.wikipedia.org/wiki/New_York_Mets\n",
      "https://bleacherreport.com/new-york-mets\n",
      "https://www.sny.tv/mets/news/\n",
      "https://nypost.com/2020/02/28/how-mets-legend-dwight-gooden-stopped-sign-stealing/\n",
      "https://www.baseball-reference.com/teams/NYM/index.shtml\n",
      "https://www.youtube.com/channel/UCgIMbGazP0uBDy9JVCqBUaA\n",
      "https://www.cbssports.com/mlb/teams/NYM/new-york-mets/\n",
      "https://www.foxnews.com/sports/mets-brodie-van-wagenen-venmo-fans\n",
      "https://www.foxsports.com/mlb/new-york-mets-team\n",
      "https://www.mlbtraderumors.com/new-york-mets\n",
      "https://www.sbnation.com/mlb/teams/new-york-mets\n",
      "https://www.stubhub.com/new-york-mets-tickets/performer/5649/\n",
      "https://www.mlbshop.com/new-york-mets/t-36556586%2Bz-9173839-345902959\n",
      "https://www.amazinavenue.com/\n",
      "https://www.tmz.com/2020/02/27/new-york-mets-grant-wish-fan-cystic-fibrosis/\n",
      "https://www.nj.com/mets/\n",
      "https://elitesportsny.com/new-york-mets/\n",
      "https://www.yardbarker.com/mlb/teams/new_york_mets/19\n",
      "https://risingapple.com/\n",
      "https://empiresportsmedia.com/category/new-york-mets/\n",
      "https://apnews.com/NewYorkMets\n",
      "https://www.instagram.com/mets/%3Fhl%3Den\n",
      "https://www.newsday.com/sports/baseball/mets\n",
      "https://www.britannica.com/topic/New-York-Mets\n",
      "https://www.nydailynews.com/sports/baseball/mets/\n",
      "https://www.nytimes.com/topic/organization/new-york-mets\n",
      "https://www.fangraphs.com/teams/mets\n",
      "https://abc7ny.com/mets/\n",
      "https://www.tcpalm.com/story/news/local/st-lucie-county/2020/02/26/new-york-mets-start-spring-training-psls-renovated-clover-park/4859628002/\n",
      "https://www.nycgo.com/events/new-york-mets-baseball\n",
      "https://www.rotoworld.com/baseball/mlb/teams/nym/new-york-mets\n",
      "https://metsmerizedonline.com/\n",
      "https://www.fanatics.com/mlb/new-york-mets/o-4521%2Bt-36889897%2Bz-96707-2229026929\n",
      "https://www.tickpick.com/new-york-mets-tickets/\n",
      "https://www.lids.com/mlb-new-york-mets/o-1209%2Bt-81660986%2Bz-94722-75745714\n",
      "https://dmv.ny.gov/plates/new-york-mets\n",
      "https://www.tesla.com/\n",
      "https://en.wikipedia.org/wiki/Tesla,_Inc.\n",
      "https://www.wired.com/tag/tesla/\n",
      "https://www.businessinsider.com/power-line-teslas-solar-slide-and-the-top-battery-industries-2020-2\n",
      "https://twitter.com/tesla%3Flang%3Den\n",
      "https://www.youtube.com/channel/UC5WjFrtBdufl6CZojX3D8dQ\n",
      "https://www.caranddriver.com/tesla\n",
      "https://www.fool.com/investing/2020/02/27/why-tesla-stock-fell-sharply-thursday.aspx\n",
      "https://teslatheband.com/\n",
      "https://hbr.org/2020/02/how-tesla-sets-itself-apart\n",
      "https://www.motor1.com/tesla/\n",
      "https://www.nytimes.com/topic/company/tesla-motors-inc\n",
      "https://www.autoblog.com/tesla/\n",
      "https://cleantechnica.com/2020/02/28/kevin-rooke-the-genius-of-tesla-superchargers/\n",
      "https://www.marketwatch.com/investing/stock/tsla\n",
      "https://electrek.co/guides/tesla/\n",
      "https://www.bloomberg.com/quote/TSLA:US\n",
      "https://www.cnbc.com/quotes/%3Fsymbol%3DTSLA\n",
      "https://finance.yahoo.com/quote/TSLA/\n",
      "https://www.wsj.com/market-data/quotes/TSLA\n",
      "https://www.consumerreports.org/cars/tesla/\n",
      "https://www.instagram.com/teslamotors/%3Fhl%3Den\n",
      "https://teslamotorsclub.com/\n",
      "https://www.foxnews.com/category/auto/make/tesla\n",
      "https://theoatmeal.com/comics/tesla\n",
      "https://www.theverge.com/tesla\n",
      "https://www.independent.co.uk/topic/tesla\n",
      "https://www.digitaltrends.com/topic/tesla/\n",
      "https://mashable.com/category/tesla-motors/\n",
      "https://vimeo.com/teslamotors\n",
      "https://www.crunchbase.com/organization/tesla-motors\n",
      "https://www.barrons.com/quote/stock/us/xnas/tsla\n",
      "https://www.facebook.com/tesla/\n",
      "https://www.nasdaq.com/market-activity/stocks/tsla\n",
      "https://www.teslaforecast.com/\n",
      "https://www.statista.com/topics/2086/tesla/\n",
      "https://www.motortrend.com/cars/tesla/\n",
      "https://www.bbc.com/news/technology-51645566\n",
      "https://www.foodnetwork.com/how-to/articles/how-to-fry-eggs-a-step-by-step-guide\n",
      "https://www.delish.com/cooking/recipe-ideas/a23499380/how-to-fry-an-egg/\n",
      "https://www.thekitchn.com/how-to-fry-an-egg-cooking-lessons-from-the-kitchn-93632\n",
      "https://www.bonappetit.com/story/how-to-make-crispy-olive-oil-fried-egg\n",
      "https://www.gimmesomeoven.com/how-to-make-fried-eggs/\n",
      "https://cookieandkate.com/favorite-fried-eggs-recipe/\n",
      "https://food52.com/blog/24802-how-to-fry-eggs\n",
      "https://www.eggs.ca/eggs101/view/7/how-to-fry-the-perfect-egg\n",
      "https://www.eggs.ca/recipes/basic-fried-eggs\n",
      "https://www.allrecipes.com/video/15/how-to-fry-an-egg/\n",
      "https://www.bbcgoodfood.com/howto/guide/how-fry-egg\n",
      "https://www.plated.com/morsel/8-perfect-ways-fry-egg/\n",
      "https://www.theguardian.com/lifeandstyle/wordofmouth/2012/nov/08/how-to-cook-the-perfect-fried-egg\n",
      "https://www.food.com/how-to/fry-an-egg-101\n",
      "https://www.wikihow.com/Fry-an-Egg\n"
     ]
    }
   ],
   "source": [
    "get_ques(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stem = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    " def clean_data(var, the_path_in, dir_name_in, stop_words_in):\n",
    "        f = open(the_path_in + dir_name_in + '/' + var, \"r\",\n",
    "                 encoding='ISO-8859-1')\n",
    "        tmp_read = re.sub('[^a-zA-Z]+', ' ', str(f.read()))\n",
    "        f.close()\n",
    "        \n",
    "        g = open(the_path_in + dir_name_in + '/' + var, \"r\",\n",
    "                 encoding='ISO-8859-1')\n",
    "        tmp_read2 = str(g.read())\n",
    "        g.close()\n",
    "        \n",
    "        tmp_read = [word.lower() for word in tmp_read.split() if word not in stop_words_in]\n",
    "        tmp_read = [my_stem.stem(word) for word in tmp_read]\n",
    "        tmp_read = ' '.join(tmp_read)\n",
    "        tmp_read2 = [word for word in tmp_read2.split()]\n",
    "        tmp_read2 = ' '.join(tmp_read2)\n",
    "        return tmp_read,tmp_read2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_path = 'D:/QMSS/Spring/NaturalLanguageProcessing/Homework/HW2/Crawled/'\n",
    "\n",
    "the_dirs = os.listdir(the_path)\n",
    "the_df = pd.DataFrame()\n",
    "for dir_name in the_dirs:\n",
    "    the_filenames = os.listdir(the_path + dir_name)\n",
    "    for word in the_filenames:\n",
    "        tmp_read = clean_data(word, the_path, dir_name, stop_words)\n",
    "        tmp = pd.DataFrame([tmp_read], columns=['body_stem', 'basic_body'])\n",
    "        tmp['label'] = dir_name\n",
    "        the_df = the_df.append(tmp, ignore_index=True)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_stem</th>\n",
       "      <th>basic_body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>these step step tip teach make fri egg whether...</td>\n",
       "      <td>These step by step tips will teach you how to ...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>delish editor handpick everi product featur we...</td>\n",
       "      <td>Delish editors handpick every product we featu...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we experienc problem retriev page request inci...</td>\n",
       "      <td>We ve experienced a problem retrieving the pag...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ordinarili hear merit fri egg base prefer fri ...</td>\n",
       "      <td>Ordinarily you hear about the merits of fried ...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>felic cloak wed nov est first publish wed nov ...</td>\n",
       "      <td>Felicity Cloake Wed 7 Nov 2012 19 10 EST First...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stump dinner get life save dinner daili newsle...</td>\n",
       "      <td>Stumped for dinner Get our life saving Dinner ...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>flip fri egg one intimid act beginn breakfast ...</td>\n",
       "      <td>Flipping your fried eggs is one of the most in...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>for better experi delia onlin websit enabl jav...</td>\n",
       "      <td>For a better experience on Delia Online websit...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>easi add littl oil fri pan crack egg rounder s...</td>\n",
       "      <td>Easy Add a little oil to a frying pan and crac...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>my favorit way egg fri i often coupl time week...</td>\n",
       "      <td>My favorite way of having eggs is fried I don ...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>save newsstand price everi fourth juli america...</td>\n",
       "      <td>Save 84 off the newsstand price Every Fourth o...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>you know fri egg i sure good excus between lea...</td>\n",
       "      <td>You don t know how to fry an egg I m sure you ...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>access page deni believ use autom tool brows w...</td>\n",
       "      <td>Access to this page has been denied because we...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>popular topic brows recip categori recip brand...</td>\n",
       "      <td>Popular Topics browse recipes categories recip...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>get crackin perfect egg fri skill week chipotl...</td>\n",
       "      <td>Get Crackin Perfect your egg frying skills thi...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>fri egg easi boil water it easi understand peo...</td>\n",
       "      <td>Frying an egg can be as easy as boiling water ...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>well probabl know understand mani fri egg fail...</td>\n",
       "      <td>Well you probably know how to do this but you ...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>a perfectli fri egg greasi it white firm rubbe...</td>\n",
       "      <td>A perfectly fried egg is not too greasy Its wh...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>view comment comment great stuff fri egg funda...</td>\n",
       "      <td>View Comments 13 Comments Great stuff Frying e...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>you tryna tricki that email look right by ad e...</td>\n",
       "      <td>You tryna be tricky That email doesn t look ri...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>a love look fri egg a love look fri egg</td>\n",
       "      <td>A lovely looking fried egg A lovely looking fr...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>simpl food simpli delici the perfectli cook eg...</td>\n",
       "      <td>Simple Food Simply Delicious The perfectly coo...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>we consult team licens nutritionist dietitian ...</td>\n",
       "      <td>We ve consulted with our team of licensed nutr...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>fri egg crispi runni perfect everyth by chri m...</td>\n",
       "      <td>Fried eggs crispy and runny and perfect on eve...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>fri egg quick versatil delici they aposr great...</td>\n",
       "      <td>Fried eggs are quick versatile and delicious T...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>gif tutori how fri an egg egg shop chef nick k...</td>\n",
       "      <td>GIF Tutorial How to Fry An Egg with Egg Shop C...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>back everyday mysteri page ye theoret but actu...</td>\n",
       "      <td>Back to Everyday Mysteries page Yes theoretica...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>hi web browser javascript disabl as result sev...</td>\n",
       "      <td>Hi this web browser has Javascript disabled As...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>if site owner manag site pleas whitelist ip th...</td>\n",
       "      <td>If you are the site owner or you manage this s...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>good cheap eat eat well budget easi recip jess...</td>\n",
       "      <td>Good Cheap Eats eat well on a budget with easy...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>home ec skill everyday live june heather solo ...</td>\n",
       "      <td>Home Ec 101 Skills for everyday living June 27...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>look sweet cooki recip latest fashion trend in...</td>\n",
       "      <td>Looking for sweet cookie recipes the latest fa...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>we may earn money link page recommend product ...</td>\n",
       "      <td>We may earn money from links on this page but ...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>how fri egg xp trubador to start need system m...</td>\n",
       "      <td>How to fry an egg on an XP by Trubador To star...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>thi post may contain affili link pleas read di...</td>\n",
       "      <td>This post may contain affiliate links Please r...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>updat it true make omelet need break egg start...</td>\n",
       "      <td>Updated 08 14 19 It s true that to make an ome...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>thi commun recip upload user topwithcinnamon f...</td>\n",
       "      <td>This Community Recipe was uploaded by the user...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>learn fri egg import one nicest thing simpl he...</td>\n",
       "      <td>Learning how to fry an egg is very important O...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>cooki kate whole food vegetarian recip blog co...</td>\n",
       "      <td>Cookie and Kate Whole Foods and Vegetarian Rec...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>columnist ella quittner never want eat anoth e...</td>\n",
       "      <td>Columnist Ella Quittner never wants to eat ano...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>think meal ingredi how mani egg eat week need ...</td>\n",
       "      <td>Thinking of all meals and ingredients How many...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>think meal ingredi how mani egg eat week need ...</td>\n",
       "      <td>Thinking of all meals and ingredients How many...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>fri egg quick easi seemingli effortless right ...</td>\n",
       "      <td>Frying eggs is quick and easy Seemingly effort...</td>\n",
       "      <td>how_to_fry_an_egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>you search twitter use search box return homepag</td>\n",
       "      <td>You can search Twitter using the search box be...</td>\n",
       "      <td>new_york_mets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>the met nation squar mlb game week live youtub...</td>\n",
       "      <td>The Mets and Nationals are squaring off in the...</td>\n",
       "      <td>new_york_mets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>billionair steve cohen reportedli still want b...</td>\n",
       "      <td>Billionaire Steve Cohen reportedly still wants...</td>\n",
       "      <td>new_york_mets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>sign like video comment subscrib load load loa...</td>\n",
       "      <td>Sign in to like videos comment and subscribe L...</td>\n",
       "      <td>new_york_mets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>via sport logo net about logo team name new yo...</td>\n",
       "      <td>via Sports Logos net About logos Team Name New...</td>\n",
       "      <td>new_york_mets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>via sport logo net about logo record rd place ...</td>\n",
       "      <td>via Sports Logos net About logos Record 86 76 ...</td>\n",
       "      <td>new_york_mets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>play now fantasi basebal play now fantasi bask...</td>\n",
       "      <td>Play Now FANTASY BASEBALL Play Now FANTASY BAS...</td>\n",
       "      <td>new_york_mets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            body_stem  \\\n",
       "0   these step step tip teach make fri egg whether...   \n",
       "1   delish editor handpick everi product featur we...   \n",
       "2   we experienc problem retriev page request inci...   \n",
       "3   ordinarili hear merit fri egg base prefer fri ...   \n",
       "4   felic cloak wed nov est first publish wed nov ...   \n",
       "5   stump dinner get life save dinner daili newsle...   \n",
       "6   flip fri egg one intimid act beginn breakfast ...   \n",
       "7   for better experi delia onlin websit enabl jav...   \n",
       "8   easi add littl oil fri pan crack egg rounder s...   \n",
       "9   my favorit way egg fri i often coupl time week...   \n",
       "10  save newsstand price everi fourth juli america...   \n",
       "11  you know fri egg i sure good excus between lea...   \n",
       "12  access page deni believ use autom tool brows w...   \n",
       "13  popular topic brows recip categori recip brand...   \n",
       "14  get crackin perfect egg fri skill week chipotl...   \n",
       "15  fri egg easi boil water it easi understand peo...   \n",
       "16  well probabl know understand mani fri egg fail...   \n",
       "17  a perfectli fri egg greasi it white firm rubbe...   \n",
       "18  view comment comment great stuff fri egg funda...   \n",
       "19  you tryna tricki that email look right by ad e...   \n",
       "20            a love look fri egg a love look fri egg   \n",
       "21  simpl food simpli delici the perfectli cook eg...   \n",
       "22  we consult team licens nutritionist dietitian ...   \n",
       "23  fri egg crispi runni perfect everyth by chri m...   \n",
       "24  fri egg quick versatil delici they aposr great...   \n",
       "25  gif tutori how fri an egg egg shop chef nick k...   \n",
       "26  back everyday mysteri page ye theoret but actu...   \n",
       "27  hi web browser javascript disabl as result sev...   \n",
       "28  if site owner manag site pleas whitelist ip th...   \n",
       "29  good cheap eat eat well budget easi recip jess...   \n",
       "30  home ec skill everyday live june heather solo ...   \n",
       "31  look sweet cooki recip latest fashion trend in...   \n",
       "32  we may earn money link page recommend product ...   \n",
       "33  how fri egg xp trubador to start need system m...   \n",
       "34  thi post may contain affili link pleas read di...   \n",
       "35  updat it true make omelet need break egg start...   \n",
       "36  thi commun recip upload user topwithcinnamon f...   \n",
       "37  learn fri egg import one nicest thing simpl he...   \n",
       "38  cooki kate whole food vegetarian recip blog co...   \n",
       "39  columnist ella quittner never want eat anoth e...   \n",
       "40  think meal ingredi how mani egg eat week need ...   \n",
       "41  think meal ingredi how mani egg eat week need ...   \n",
       "42  fri egg quick easi seemingli effortless right ...   \n",
       "43   you search twitter use search box return homepag   \n",
       "44  the met nation squar mlb game week live youtub...   \n",
       "45  billionair steve cohen reportedli still want b...   \n",
       "46  sign like video comment subscrib load load loa...   \n",
       "47  via sport logo net about logo team name new yo...   \n",
       "48  via sport logo net about logo record rd place ...   \n",
       "49  play now fantasi basebal play now fantasi bask...   \n",
       "\n",
       "                                           basic_body              label  \n",
       "0   These step by step tips will teach you how to ...  how_to_fry_an_egg  \n",
       "1   Delish editors handpick every product we featu...  how_to_fry_an_egg  \n",
       "2   We ve experienced a problem retrieving the pag...  how_to_fry_an_egg  \n",
       "3   Ordinarily you hear about the merits of fried ...  how_to_fry_an_egg  \n",
       "4   Felicity Cloake Wed 7 Nov 2012 19 10 EST First...  how_to_fry_an_egg  \n",
       "5   Stumped for dinner Get our life saving Dinner ...  how_to_fry_an_egg  \n",
       "6   Flipping your fried eggs is one of the most in...  how_to_fry_an_egg  \n",
       "7   For a better experience on Delia Online websit...  how_to_fry_an_egg  \n",
       "8   Easy Add a little oil to a frying pan and crac...  how_to_fry_an_egg  \n",
       "9   My favorite way of having eggs is fried I don ...  how_to_fry_an_egg  \n",
       "10  Save 84 off the newsstand price Every Fourth o...  how_to_fry_an_egg  \n",
       "11  You don t know how to fry an egg I m sure you ...  how_to_fry_an_egg  \n",
       "12  Access to this page has been denied because we...  how_to_fry_an_egg  \n",
       "13  Popular Topics browse recipes categories recip...  how_to_fry_an_egg  \n",
       "14  Get Crackin Perfect your egg frying skills thi...  how_to_fry_an_egg  \n",
       "15  Frying an egg can be as easy as boiling water ...  how_to_fry_an_egg  \n",
       "16  Well you probably know how to do this but you ...  how_to_fry_an_egg  \n",
       "17  A perfectly fried egg is not too greasy Its wh...  how_to_fry_an_egg  \n",
       "18  View Comments 13 Comments Great stuff Frying e...  how_to_fry_an_egg  \n",
       "19  You tryna be tricky That email doesn t look ri...  how_to_fry_an_egg  \n",
       "20  A lovely looking fried egg A lovely looking fr...  how_to_fry_an_egg  \n",
       "21  Simple Food Simply Delicious The perfectly coo...  how_to_fry_an_egg  \n",
       "22  We ve consulted with our team of licensed nutr...  how_to_fry_an_egg  \n",
       "23  Fried eggs crispy and runny and perfect on eve...  how_to_fry_an_egg  \n",
       "24  Fried eggs are quick versatile and delicious T...  how_to_fry_an_egg  \n",
       "25  GIF Tutorial How to Fry An Egg with Egg Shop C...  how_to_fry_an_egg  \n",
       "26  Back to Everyday Mysteries page Yes theoretica...  how_to_fry_an_egg  \n",
       "27  Hi this web browser has Javascript disabled As...  how_to_fry_an_egg  \n",
       "28  If you are the site owner or you manage this s...  how_to_fry_an_egg  \n",
       "29  Good Cheap Eats eat well on a budget with easy...  how_to_fry_an_egg  \n",
       "30  Home Ec 101 Skills for everyday living June 27...  how_to_fry_an_egg  \n",
       "31  Looking for sweet cookie recipes the latest fa...  how_to_fry_an_egg  \n",
       "32  We may earn money from links on this page but ...  how_to_fry_an_egg  \n",
       "33  How to fry an egg on an XP by Trubador To star...  how_to_fry_an_egg  \n",
       "34  This post may contain affiliate links Please r...  how_to_fry_an_egg  \n",
       "35  Updated 08 14 19 It s true that to make an ome...  how_to_fry_an_egg  \n",
       "36  This Community Recipe was uploaded by the user...  how_to_fry_an_egg  \n",
       "37  Learning how to fry an egg is very important O...  how_to_fry_an_egg  \n",
       "38  Cookie and Kate Whole Foods and Vegetarian Rec...  how_to_fry_an_egg  \n",
       "39  Columnist Ella Quittner never wants to eat ano...  how_to_fry_an_egg  \n",
       "40  Thinking of all meals and ingredients How many...  how_to_fry_an_egg  \n",
       "41  Thinking of all meals and ingredients How many...  how_to_fry_an_egg  \n",
       "42  Frying eggs is quick and easy Seemingly effort...  how_to_fry_an_egg  \n",
       "43  You can search Twitter using the search box be...      new_york_mets  \n",
       "44  The Mets and Nationals are squaring off in the...      new_york_mets  \n",
       "45  Billionaire Steve Cohen reportedly still wants...      new_york_mets  \n",
       "46  Sign in to like videos comment and subscribe L...      new_york_mets  \n",
       "47  via Sports Logos net About logos Team Name New...      new_york_mets  \n",
       "48  via Sports Logos net About logos Record 86 76 ...      new_york_mets  \n",
       "49  Play Now FANTASY BASEBALL Play Now FANTASY BAS...      new_york_mets  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_df.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
